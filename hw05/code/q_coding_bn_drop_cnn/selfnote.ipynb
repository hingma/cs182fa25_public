{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225fc70f",
   "metadata": {},
   "source": [
    "I'll show you the complete mathematical formulation for the batch normalization backward pass. Let me create a clear step-by-step derivation:\n",
    "\n",
    "## Batch Normalization Backward Pass - Mathematical Formulation\n",
    "\n",
    "### Forward Pass Recap\n",
    "\n",
    "Given input $\\mathbf{x} \\in \\mathbb{R}^{N \\times D}$, the forward pass computes:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu_j &= \\frac{1}{N} \\sum_{i=1}^N x_{i,j} && \\text{(mean)} \\\\\n",
    "\\sigma^2_j &= \\frac{1}{N} \\sum_{i=1}^N (x_{i,j} - \\mu_j)^2 && \\text{(variance)} \\\\\n",
    "\\hat{x}_{i,j} &= \\frac{x_{i,j} - \\mu_j}{\\sqrt{\\sigma^2_j + \\epsilon}} && \\text{(normalization)} \\\\\n",
    "y_{i,j} &= \\gamma_j \\hat{x}_{i,j} + \\beta_j && \\text{(scale and shift)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Backward Pass Derivation\n",
    "\n",
    "Given upstream gradient $\\frac{\\partial \\mathcal{L}}{\\partial y_{i,j}}$ (denoted as $dy_{i,j}$ or `dout`), compute:\n",
    "\n",
    "#### **Step 1: Gradient w.r.t. β (shift parameter)**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\beta_j} = \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}}{\\partial y_{i,j}} = \\sum_{i=1}^N dy_{i,j}$$\n",
    "\n",
    "**Code:** `dbeta = np.sum(dout, axis=0)`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Gradient w.r.t. γ (scale parameter)**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\gamma_j} = \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}}{\\partial y_{i,j}} \\cdot \\frac{\\partial y_{i,j}}{\\partial \\gamma_j} = \\sum_{i=1}^N dy_{i,j} \\cdot \\hat{x}_{i,j}$$\n",
    "\n",
    "**Code:** `dgamma = np.sum(dout * x_norm, axis=0)`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Gradient w.r.t. normalized x**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial y_{i,j}} \\cdot \\frac{\\partial y_{i,j}}{\\partial \\hat{x}_{i,j}} = dy_{i,j} \\cdot \\gamma_j$$\n",
    "\n",
    "**Code:** `dx_norm = dout * gamma`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Gradient w.r.t. variance**\n",
    "\n",
    "Since $\\hat{x}_{i,j} = (x_{i,j} - \\mu_j) \\cdot (\\sigma^2_j + \\epsilon)^{-1/2}$:\n",
    "\n",
    "$$\\frac{\\partial \\hat{x}_{i,j}}{\\partial \\sigma^2_j} = (x_{i,j} - \\mu_j) \\cdot \\left(-\\frac{1}{2}\\right)(\\sigma^2_j + \\epsilon)^{-3/2}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2_j} = \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} \\cdot (x_{i,j} - \\mu_j) \\cdot \\left(-\\frac{1}{2}\\right)(\\sigma^2_j + \\epsilon)^{-3/2}$$\n",
    "\n",
    "**Code:** `dvar = np.sum(dx_norm * x_centered, axis=0) * -0.5 * (sample_var + eps) ** (-1.5)`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Gradient w.r.t. mean**\n",
    "\n",
    "The mean $\\mu_j$ affects the loss through TWO paths:\n",
    "1. Direct path through $\\hat{x}_{i,j}$\n",
    "2. Indirect path through $\\sigma^2_j$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu_j} = \\underbrace{\\sum_{i=1}^N \\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} \\cdot \\frac{\\partial \\hat{x}_{i,j}}{\\partial \\mu_j}}_{\\text{direct}} + \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2_j} \\cdot \\frac{\\partial \\sigma^2_j}{\\partial \\mu_j}}_{\\text{through variance}}$$\n",
    "\n",
    "Where:\n",
    "- $\\frac{\\partial \\hat{x}_{i,j}}{\\partial \\mu_j} = -(\\sigma^2_j + \\epsilon)^{-1/2}$\n",
    "- $\\frac{\\partial \\sigma^2_j}{\\partial \\mu_j} = -\\frac{2}{N}\\sum_{i=1}^N (x_{i,j} - \\mu_j)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mu_j} = \\sum_{i=1}^N \\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} \\cdot \\left(-\\frac{1}{\\sqrt{\\sigma^2_j + \\epsilon}}\\right) + \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2_j} \\cdot \\left(-\\frac{2}{N}\\sum_{i=1}^N (x_{i,j} - \\mu_j)\\right)$$\n",
    "\n",
    "**Code:** `dmu = np.sum(dx_norm * -1.0 / std, axis=0) + dvar * np.sum(-2.0 * x_centered, axis=0) / N`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 6: Gradient w.r.t. input x**\n",
    "\n",
    "The input $x_{i,j}$ affects the loss through THREE paths:\n",
    "1. Direct path through $\\hat{x}_{i,j}$\n",
    "2. Path through $\\sigma^2_j$\n",
    "3. Path through $\\mu_j$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x_{i,j}} = \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} \\cdot \\frac{\\partial \\hat{x}_{i,j}}{\\partial x_{i,j}}}_{\\text{direct}} + \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2_j} \\cdot \\frac{\\partial \\sigma^2_j}{\\partial x_{i,j}}}_{\\text{through variance}} + \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mu_j} \\cdot \\frac{\\partial \\mu_j}{\\partial x_{i,j}}}_{\\text{through mean}}$$\n",
    "\n",
    "Where:\n",
    "- $\\frac{\\partial \\hat{x}_{i,j}}{\\partial x_{i,j}} = (\\sigma^2_j + \\epsilon)^{-1/2}$\n",
    "- $\\frac{\\partial \\sigma^2_j}{\\partial x_{i,j}} = \\frac{2}{N}(x_{i,j} - \\mu_j)$\n",
    "- $\\frac{\\partial \\mu_j}{\\partial x_{i,j}} = \\frac{1}{N}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial \\mathcal{L}}{\\partial x_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{x}_{i,j}} \\cdot \\frac{1}{\\sqrt{\\sigma^2_j + \\epsilon}} + \\frac{\\partial \\mathcal{L}}{\\partial \\sigma^2_j} \\cdot \\frac{2(x_{i,j} - \\mu_j)}{N} + \\frac{\\partial \\mathcal{L}}{\\partial \\mu_j} \\cdot \\frac{1}{N}}$$\n",
    "\n",
    "**Code:** `dx = dx_norm / std + dvar * 2.0 * x_centered / N + dmu / N`\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Complete Backward Pass Algorithm\n",
    "\n",
    "```\n",
    "Input: dout (upstream gradient), cache (x, μ, σ², ε, γ, β)\n",
    "\n",
    "1. Compute intermediate values:\n",
    "   std = √(σ² + ε)\n",
    "   x_centered = x - μ\n",
    "   x_norm = x_centered / std\n",
    "\n",
    "2. Compute gradients:\n",
    "   dbeta = Σᵢ dout[i,j]\n",
    "   dgamma = Σᵢ dout[i,j] · x_norm[i,j]\n",
    "   dx_norm = dout · γ\n",
    "   dvar = Σᵢ dx_norm[i,j] · x_centered[i,j] · (-½)(σ² + ε)^(-3/2)\n",
    "   dmu = Σᵢ dx_norm[i,j] · (-1/std) + dvar · Σᵢ(-2·x_centered[i,j])/N\n",
    "   dx = dx_norm/std + dvar·2·x_centered/N + dmu/N\n",
    "\n",
    "Output: dx, dgamma, dbeta\n",
    "```\n",
    "\n",
    "This formulation correctly handles the complex dependencies in the batch normalization computational graph!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
