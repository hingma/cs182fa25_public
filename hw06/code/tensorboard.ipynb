{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Berkeley-CS182/cs182fa25_public.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cs182fa25_public/hw06/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from architectures import BasicConvNet, ResNet18, MLP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Tensorboard\n",
    "Tensorboard is a local tool for visualizing images, metrics, histograms, and more. It is designed for tensorflow, but can be integrated with torch. Let's explore tensorboard usage with an example:\n",
    "\n",
    "```from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# To start a run, call the following\n",
    "writer = SummaryWriter(comment=f'Name_of_Run')\n",
    "\n",
    "# When you want to log a value, use the writer. When adding a scalar, the format is as follows: \n",
    "# add_scalar(tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n",
    "writer.add_scalar('Training Loss', loss.item(), step)\n",
    "\n",
    "# Finally, when you are done logging values, close the writer\n",
    "writer.close()\n",
    "\n",
    "\n",
    "```\n",
    "There are many other functionalities and methods that you are free to explore, but will not be mentioned in this notebook.\n",
    "\n",
    "## Your Task\n",
    "We will be once again building classifiers for the CIFAR-10. There are various architectures set up for you to use in the architectures.py file. Using tensorboard, please search through 5 different hyperparameter configurations. Examples of choices include: learning rate, batch size, architecture, optimization algorithm, etc. Please submit the generated plots on your pdf and answer question A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2  # Reduced for demonstration, increase for better results\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 hyperparameter configurations to test\n",
    "hyperparameters = [\n",
    "    {\n",
    "        'name': 'basic_conv_sgd',\n",
    "        'model': 'BasicConvNet',\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd',\n",
    "        'weight_decay': 1e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'basic_conv_adam',\n",
    "        'model': 'BasicConvNet',\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'weight_decay': 1e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'resnet_adam',\n",
    "        'model': 'ResNet18',\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'weight_decay': 1e-4\n",
    "    },\n",
    "    {\n",
    "        'name': 'mlp_adam',\n",
    "        'model': 'MLP',\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'weight_decay': 1e-5\n",
    "    },\n",
    "    {\n",
    "        'name': 'mlp_sgd_large_batch',\n",
    "        'model': 'MLP',\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd',\n",
    "        'weight_decay': 1e-4\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hp):\n",
    "    # Create TensorBoard writer\n",
    "    writer = SummaryWriter(comment=f\"_{hp['name']}\")\n",
    "    \n",
    "    # Set up data loaders\n",
    "    batch_size = hp['batch_size']\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Initialize model based on hyperparameter configuration\n",
    "    if hp['model'] == 'BasicConvNet':\n",
    "        model = BasicConvNet()\n",
    "    elif hp['model'] == 'ResNet18':\n",
    "        # For ResNet18, we need to resize images to 224x224\n",
    "        transform_224 = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        trainset.transform = transform_224\n",
    "        testset.transform = transform_224\n",
    "        model = ResNet18()\n",
    "    elif hp['model'] == 'MLP':\n",
    "        model = MLP()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {hp['model']}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    if hp['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=hp['learning_rate'], momentum=0.9, weight_decay=hp['weight_decay'])\n",
    "    elif hp['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=hp['learning_rate'], weight_decay=hp['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {hp['optimizer']}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training\n",
    "        train_loop = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\")\n",
    "        for i, data in enumerate(train_loop):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # MLP expects flattened input\n",
    "            if hp['model'] == 'MLP':\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_loop.set_postfix({\n",
    "                'loss': running_loss / (i + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "            \n",
    "            # Log to TensorBoard (every 100 batches)\n",
    "            if i % 100 == 99:\n",
    "                step = epoch * len(trainloader) + i\n",
    "                writer.add_scalar('Training Loss', running_loss / 100, step)\n",
    "                writer.add_scalar('Training Accuracy', 100. * correct / total, step)\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_loop = tqdm(testloader, desc=f\"Epoch {epoch+1}/{epochs} (Test)\")\n",
    "            for i, data in enumerate(test_loop):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                \n",
    "                # MLP expects flattened input\n",
    "                if hp['model'] == 'MLP':\n",
    "                    inputs = inputs.view(inputs.size(0), -1)\n",
    "                    \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                test_loop.set_postfix({\n",
    "                    'loss': test_loss / (i + 1),\n",
    "                    'acc': 100. * correct / total\n",
    "                })\n",
    "        \n",
    "        # Log epoch statistics to TensorBoard\n",
    "        writer.add_scalar('Test Loss', test_loss / len(testloader), epoch)\n",
    "        writer.add_scalar('Test Accuracy', 100. * correct / total, epoch)\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"Training complete for {hp['name']}\")\n",
    "\n",
    "def run():\n",
    "    for hp in hyperparameters:\n",
    "        print(f\"\\nStarting training with configuration: {hp['name']}\")\n",
    "        train_model(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view tensorboard results:\n",
    "# Run this in terminal or a separate cell:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=runs\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
