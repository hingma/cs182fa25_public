{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2b09c3",
   "metadata": {},
   "source": [
    "Based on the notebook content, the symmetric normalized adjacency matrix, often denoted as $\\tilde{A}^{SymNorm}$ in graph neural networks literature, is calculated as:\n",
    "\n",
    "$$\\tilde{A}^{SymNorm} = D^{-1/2} \\tilde{A} D^{-1/2}$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{A}$ is the adjacency matrix with self-loops added: $\\tilde{A} = A + I$ \n",
    "  - $A$ is the original adjacency matrix\n",
    "  - $I$ is the identity matrix\n",
    "- $D$ is the degree matrix of $\\tilde{A}$ (diagonal matrix with node degrees on the diagonal)\n",
    "- $D^{-1/2}$ is the inverse square root of the degree matrix\n",
    "\n",
    "In implementation, this involves:\n",
    "\n",
    "1. Adding self-loops: $\\tilde{A} = A + I$\n",
    "2. Computing the degree matrix: $D = \\text{diag}(\\sum_j \\tilde{A}_{ij})$\n",
    "3. Computing $D^{-1/2}$: taking inverse square root of each diagonal element\n",
    "4. Computing $\\tilde{A}^{SymNorm} = D^{-1/2} \\tilde{A} D^{-1/2}$\n",
    "\n",
    "This normalization is crucial in graph neural networks as it prevents numerical instabilities from varying node degrees and helps control the scale of features being passed through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9768d9f",
   "metadata": {},
   "source": [
    "The symmetric normalized adjacency matrix ($\\tilde{A}^{SymNorm} = D^{-1/2} \\tilde{A} D^{-1/2}$) has several important implications in graph neural networks:\n",
    "\n",
    "1. **Balance of influence**: It prevents high-degree nodes (those with many connections) from dominating the message passing process by scaling each node's contribution according to the square root of its degree.\n",
    "\n",
    "2. **Numerical stability**: Without normalization, repeatedly applying the adjacency matrix in deep GNNs could lead to numerical instability with features either exploding or vanishing.\n",
    "\n",
    "3. **Spectral properties**: The symmetric normalization preserves eigenvalue properties that are beneficial for learning graph representations and keeps them within a bounded range.\n",
    "\n",
    "4. **Equal contribution**: It ensures that information from both high-degree and low-degree nodes contributes more equally to the graph learning process.\n",
    "\n",
    "5. **Signal dampening**: It dampens the signal propagation in highly connected regions of the graph, preventing over-smoothing in deeper GNN architectures.\n",
    "\n",
    "This normalization is a key component of why Graph Convolutional Networks work effectively for node classification tasks like the Zachary's Karate Club example in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af943a4",
   "metadata": {},
   "source": [
    "# `Muon` is MuP + orthogonalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55235515",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a222b7cf",
   "metadata": {},
   "source": [
    "### The integration of Adam and muP\n",
    "The integration of Adam and muP in AdamWMuP happens at a specific point in the optimization process:\n",
    "\n",
    "1. **Standard Adam steps are performed first**:\n",
    "   - Maintain momentum (m) and variance (v) estimates of gradients\n",
    "   - Apply bias correction to get m_hat and v_hat\n",
    "   - Compute the update direction: $u = m_{hat} / (\\sqrt{v_{hat}} + \\epsilon)$\n",
    "\n",
    "2. **Then muP scaling is applied**:\n",
    "   - For matrix parameters (weights), reshape to 2D if needed\n",
    "   - Apply the muP scaling factor: $\\tilde{u} = u *\\sqrt{max(1, d_{out}/d_{in})}$\n",
    "   - Reshape back to original format if needed\n",
    "\n",
    "3. **Finally, the parameter update happens**:\n",
    "   - Apply the scaled update: $p -= lr * \\tilde{u}$\n",
    "\n",
    "Looking at the code in the AdamWMuP class (lines 410-459), you can see this exact workflow - all Adam computations happen normally, and just before applying the update to parameters, the muP scaling is applied to matrix parameters.\n",
    "\n",
    "This approach cleverly preserves the adaptive learning rate benefits of Adam while adding the dimension-based scaling benefits of muP, giving us the \"best of both worlds\" without fundamentally changing either algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8596e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
