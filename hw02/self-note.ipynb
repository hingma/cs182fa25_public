{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69694432",
   "metadata": {},
   "source": [
    "No, not exactly. Let's break down the weight matrices in the context of the neural network formulation:\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{W}^{(2)} \\Phi \\left( \\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)} \\right) + \\mathbf{b}^{(2)} $$\n",
    "\n",
    "1.  **$\\mathbf{W}^{(1)}$ (First Layer Weight Matrix)**:\n",
    "    *   This matrix has dimensions `width` $\\times$ `1`.\n",
    "    *   Each **row** of $\\mathbf{W}^{(1)}$ represents the weight(s) connecting the **input `x` to a single neuron in the hidden layer**.\n",
    "    *   Since `x` is a scalar (1-dimensional), each row will only contain a single weight. So, if `width` is 10, $\\mathbf{W}^{(1)}$ will be a 10x1 matrix, and each of its 10 rows will have one weight.\n",
    "\n",
    "2.  **$\\mathbf{W}^{(2)}$ (Output Layer Weight Matrix)**:\n",
    "    *   This matrix has dimensions `1` $\\times$ `width`.\n",
    "    *   Each **column** of $\\mathbf{W}^{(2)}$ represents the weight connecting a **single neuron in the hidden layer to the output neuron**.\n",
    "    *   So, if `width` is 10, $\\mathbf{W}^{(2)}$ will be a 1x10 matrix, and each of its 10 columns will have one weight.\n",
    "\n",
    "In summary:\n",
    "*   For $\\mathbf{W}^{(1)}$, a \"group of weights\" (specifically, the weights associated with one hidden neuron) would be a **row** of $\\mathbf{W}^{(1)}$.\n",
    "*   For $\\mathbf{W}^{(2)}$, a \"group of weights\" (specifically, the weights associated with one hidden neuron's contribution to the output) would be a **column** of $\\mathbf{W}^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdaa365",
   "metadata": {},
   "source": [
    "# X as matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7e62b",
   "metadata": {},
   "source": [
    "When the data `X` is multi-dimensional and you have a lot of data points, `X` becomes a matrix. Let's say:\n",
    "\n",
    "*   `n_samples` is the number of data points.\n",
    "*   `n_features` is the dimensionality of each data point (e.g., if each `x` is a 2D vector, `n_features` = 2).\n",
    "*   `width` is the number of neurons in the hidden layer.\n",
    "\n",
    "The input data `X` would be a matrix of shape `(n_samples, n_features)`.\n",
    "\n",
    "The neural network formulation adapts as follows:\n",
    "\n",
    "1.  **First Layer (Input to Hidden Layer)**:\n",
    "    *   The input to the first linear layer will be the `X` matrix of shape `(n_samples, n_features)`.\n",
    "    *   The weight matrix $\\mathbf{W}^{(1)}$ will now have dimensions `(width, n_features)`. Each row of this matrix represents the weights connecting *all* `n_features` of the input to a single hidden neuron.\n",
    "    *   The bias vector $\\mathbf{b}^{(1)}$ will still have `width` elements, one for each hidden neuron, with a shape of `(width,)`.\n",
    "    *   The operation performed by the first linear layer, conceptually, would be equivalent to:\n",
    "        `Z_1 = X @ W_transpose_1 + b_1`\n",
    "        where `W_transpose_1` has shape `(n_features, width)`.\n",
    "        The result `Z_1` will be a matrix of shape `(n_samples, width)`. Each row of `Z_1` contains the pre-activation values for one data point across all `width` hidden neurons.\n",
    "\n",
    "2.  **Activation Function**:\n",
    "    *   The ReLU activation function $\\Phi$ is applied element-wise to `Z_1`, resulting in `H_1` of shape `(n_samples, width)`.\n",
    "\n",
    "3.  **Second Layer (Hidden to Output Layer)**:\n",
    "    *   The input to this layer is `H_1`, with shape `(n_samples, width)`.\n",
    "    *   The weight matrix $\\mathbf{W}^{(2)}$ will still have dimensions `(1, width)`, as it transforms the `width`-dimensional hidden representation into a single output value for each sample. Each column of this matrix represents the weight connecting a single hidden neuron to the output.\n",
    "    *   The bias $\\mathbf{b}^{(2)}$ remains a single scalar.\n",
    "    *   The operation for the output layer would be:\n",
    "        `Y_hat = H_1 @ W_transpose_2 + b_2`\n",
    "        where `W_transpose_2` has shape `(width, 1)`.\n",
    "        The result `Y_hat` will be a matrix of shape `(n_samples, 1)`, where each row is the predicted output for the corresponding data point.\n",
    "\n",
    "In essence, the core formulation remains similar, but the operations are now performed using efficient matrix multiplications that process all `n_samples` data points simultaneously, rather than individually. The dimensions of the weight matrices are adjusted to accommodate the `n_features` of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd03b31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebbe1ae",
   "metadata": {},
   "source": [
    "That's a good conceptual question that dives into the linear algebra! Let's clarify the relationship between the input data `X` and the \"span space\" of the weight matrix $\\mathbf{W}^{(1)}$.\n",
    "\n",
    "Recall the operation of the first layer:\n",
    "\n",
    "$$ \\mathbf{Z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{X}^T + \\mathbf{b}^{(1)} $$\n",
    "\n",
    "(Note: I'm using $\\mathbf{X}^T$ here to align with standard matrix multiplication conventions where the weight matrix $\\mathbf{W}^{(1)}$ (shape `width` x `n_features`) operates on input vectors (shape `n_features` x `1`). If `X` is `n_samples` x `n_features`, then $\\mathbf{X}^T$ would be `n_features` x `n_samples`).\n",
    "\n",
    "Let's consider a single data point, represented as a column vector $x_j \\in \\mathbb{R}^{n_{features}}$. The output of the linear transformation for this data point is:\n",
    "\n",
    "$$ z_j^{(1)} = \\mathbf{W}^{(1)} x_j + \\mathbf{b}^{(1)} $$\n",
    "\n",
    "*   **Input Vector $x_j$**: This vector lives in the input feature space, which is $\\mathbb{R}^{n_{features}}$.\n",
    "*   **Weight Matrix $\\mathbf{W}^{(1)}$**: This matrix has `width` rows and `n_features` columns.\n",
    "    *   The **rows** of $\\mathbf{W}^{(1)}$ are vectors in $\\mathbb{R}^{n_{features}}$. These can be thought of as \"feature detectors\" or \"basis vectors\" that the network uses to analyze the input $x_j$.\n",
    "    *   The **columns** of $\\mathbf{W}^{(1)}$ are vectors in $\\mathbb{R}^{width}$.\n",
    "\n",
    "When you compute $\\mathbf{W}^{(1)} x_j$:\n",
    "The result is a vector in $\\mathbb{R}^{width}$. This vector is formed by taking a linear combination of the **columns** of $\\mathbf{W}^{(1)}$, where the coefficients of the linear combination are the elements of the input vector $x_j$.\n",
    "\n",
    "Therefore, the output vector $z_j^{(1)}$ (before adding bias and applying ReLU) **is in the column space of $\\mathbf{W}^{(1)}$**.\n",
    "\n",
    "However, the input vector $x_j$ itself is generally **not** considered to be \"in the span space of $\\mathbf{W}^{(1)}$\" in the same way. $x_j$ is the input that is being *transformed* by $\\mathbf{W}^{(1)}$. While $x_j$ exists in the same space as the row vectors of $\\mathbf{W}^{(1)}$ (i.e., $\\mathbb{R}^{n_{features}}$), it's not typically a linear combination of those rows.\n",
    "\n",
    "Think of it this way:\n",
    "*   The **rows of $\\mathbf{W}^{(1)}$ define a set of directions or filters** in the input space. Each element of $\\mathbf{W}^{(1)} x_j$ essentially measures how much $x_j$ activates or aligns with each of these directions.\n",
    "*   The **column space of $\\mathbf{W}^{(1)}$ is the space where the output of the linear transformation $\\mathbf{W}^{(1)} x_j$ resides**.\n",
    "\n",
    "So, to directly answer your question: No, the input vector `X` (or any individual data point $x_j$) is not generally a vector in the span space of $\\mathbf{W}^{(1)}$. Instead, $\\mathbf{W}^{(1)}$ acts as a linear transformation that maps the input vector from $\\mathbb{R}^{n_{features}}$ to a vector in the column space of $\\mathbf{W}^{(1)}$ (which is a subspace of $\\mathbb{R}^{width}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a8667",
   "metadata": {},
   "source": [
    "Q:what's the formulation of the 2-hidden layer ReLu network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e022bc",
   "metadata": {},
   "source": [
    "You're looking for the formulation of the two-hidden layer ReLU network, which is discussed in the notebook as an extension.\n",
    "\n",
    "The 2-hidden layer network implements the function:\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{W}^{(3)} \\Phi(\\mathbf{W}^{(2)} \\Phi(\\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)}) + \\mathbf{b}^{(3)} $$\n",
    "\n",
    "Here's a breakdown of each component:\n",
    "\n",
    "*   **Input Layer to First Hidden Layer:**\n",
    "    *   $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{width \\times n_{features}}$ (or $\\mathbb{R}^{width \\times 1}$ if $x$ is scalar) is the weight matrix for the first hidden layer.\n",
    "    *   $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{width}$ is the bias vector for the first hidden layer.\n",
    "    *   $\\Phi(z) = \\text{ReLU}(z) = \\max(0, z)$ is the ReLU activation function applied element-wise.\n",
    "    *   The term $\\Phi(\\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)})$ represents the activated outputs of the first hidden layer.\n",
    "\n",
    "*   **First Hidden Layer to Second Hidden Layer:**\n",
    "    *   $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{width \\times width}$ is the weight matrix connecting the first hidden layer to the second hidden layer.\n",
    "    *   $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{width}$ is the bias vector for the second hidden layer.\n",
    "    *   $\\Phi(\\mathbf{W}^{(2)} \\Phi(\\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)})$ represents the activated outputs of the second hidden layer.\n",
    "\n",
    "*   **Second Hidden Layer to Output Layer:**\n",
    "    *   $\\mathbf{W}^{(3)} \\in \\mathbb{R}^{1 \\times width}$ is the weight matrix connecting the second hidden layer to the output layer.\n",
    "    *   $\\mathbf{b}^{(3)} \\in \\mathbb{R}$ is the scalar bias for the output layer.\n",
    "\n",
    "In this formulation, both hidden layers have the same `width` (number of neurons). The network sequentially applies linear transformations followed by ReLU activations, culminating in a final linear transformation to produce the output $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d86d7",
   "metadata": {},
   "source": [
    "understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f87e89",
   "metadata": {},
   "source": [
    "I'll explain the mathematical concepts behind each major component of the code:\n",
    "\n",
    "1. **Neural Network Architecture (Single Layer)**\n",
    "The basic network implements a function:\n",
    "$$ \\hat{y} = \\mathbf{W}^{(2)} \\Phi(\\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)} $$\n",
    "where:\n",
    "- $\\Phi(x) = \\text{ReLU}(x) = \\max(0, x)$ is the activation function\n",
    "- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{width \\times 1}$ is the first layer weight matrix\n",
    "- $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{width}$ is the first layer bias vector\n",
    "- $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{1 \\times width}$ is the output layer weight matrix\n",
    "- $\\mathbf{b}^{(2)} \\in \\mathbb{R}$ is the output bias\n",
    "\n",
    "2. **Gradient Visualization (backward_and_plot_grad)**\n",
    "This function computes and visualizes:\n",
    "$$ \\frac{\\partial y}{\\partial \\theta_i}(x) $$\n",
    "where $\\theta_i$ represents any learnable parameter (weights or biases). For each parameter, it shows how the output changes with respect to that parameter across different input values x.\n",
    "\n",
    "3. **Feature Matrix and SVD Analysis**\n",
    "The feature matrix $\\mathbf{F}$ is constructed where:\n",
    "- Each row corresponds to a training point $x_j$\n",
    "- Each column corresponds to a parameter $\\theta_i$\n",
    "- Entry $F_{ji} = \\frac{\\partial y}{\\partial \\theta_i}(x_j)$\n",
    "\n",
    "So for n training points and d parameters:\n",
    "$$ \\mathbf{F} \\in \\mathbb{R}^{n \\times d} $$\n",
    "\n",
    "The SVD decomposition gives:\n",
    "$$ \\mathbf{F} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T $$\n",
    "where:\n",
    "- $\\mathbf{U} \\in \\mathbb{R}^{n \\times d}$ contains the left singular vectors\n",
    "- $\\mathbf{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix of singular values\n",
    "- $\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$ contains the right singular vectors\n",
    "\n",
    "4. **Principal Features**\n",
    "The principal features are computed as:\n",
    "$$ \\text{Principal Features} = \\mathbf{U}\\mathbf{\\Sigma} $$\n",
    "\n",
    "This gives us the weighted contribution of each feature direction, where:\n",
    "- Larger singular values indicate more important feature directions\n",
    "- Each column of $\\mathbf{U}\\mathbf{\\Sigma}$ represents a principal feature direction weighted by its importance\n",
    "\n",
    "5. **Two-Layer Network Extension**\n",
    "The two-layer network implements:\n",
    "$$ \\hat{y} = \\mathbf{W}^{(3)} \\Phi(\\mathbf{W}^{(2)} \\Phi(\\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)}) + \\mathbf{b}^{(3)} $$\n",
    "where:\n",
    "- Both hidden layers have the same width\n",
    "- $\\mathbf{W}^{(1)}, \\mathbf{W}^{(2)} \\in \\mathbb{R}^{width \\times width}$\n",
    "- $\\mathbf{b}^{(1)}, \\mathbf{b}^{(2)} \\in \\mathbb{R}^{width}$\n",
    "- $\\mathbf{W}^{(3)} \\in \\mathbb{R}^{1 \\times width}$\n",
    "- $\\mathbf{b}^{(3)} \\in \\mathbb{R}$\n",
    "\n",
    "The key insight here is that we're studying how the network linearizes around its current parameters. The feature matrix $\\mathbf{F}$ captures this local linear approximation, and its SVD reveals:\n",
    "1. The most important directions in parameter space (through $\\mathbf{V}$)\n",
    "2. The relative importance of these directions (through $\\mathbf{\\Sigma}$)\n",
    "3. How these directions manifest in the function space (through $\\mathbf{U}$)\n",
    "\n",
    "This analysis helps understand:\n",
    "- Which parameter changes have the most impact on the output\n",
    "- How the network's behavior changes from initialization to after training\n",
    "- How the network width affects the learning dynamics\n",
    "- The effective dimensionality of the learned function through the distribution of singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2c65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89265d99",
   "metadata": {},
   "source": [
    "The phenomenon you're describing, where \"the gradient of the output with respect to a specific parameter changes as the input x changes,\" is not a problem but a fundamental and expected characteristic of neural networks, especially those with non-linear activation functions.\n",
    "\n",
    "Here's a breakdown of why this happens:\n",
    "\n",
    "1.  **Non-Linearity of Neural Networks**:\n",
    "    *   Neural networks use non-linear activation functions (like ReLU in this notebook) between layers. If they were purely linear, stacking layers would just result in another linear function, and the gradients would be constant with respect to the input `x`.\n",
    "    *   The ReLU function, $\\Phi(z) = \\max(0, z)$, introduces piecewise linearity. Its derivative is 1 for $z > 0$, 0 for $z < 0$, and undefined at $z = 0$. This means the \"slope\" or gradient of the network's output with respect to its parameters will change depending on whether a neuron is active ($z>0$) or inactive ($z<0$).\n",
    "\n",
    "2.  **Chain Rule in Backpropagation**:\n",
    "    *   When computing gradients in a neural network, the chain rule is applied. The gradient of the output $y$ with respect to a parameter $\\theta_k$ in an earlier layer depends on the gradients of all subsequent layers and the intermediate activations.\n",
    "    *   For example, consider the gradient of the output $\\hat{y}$ with respect to a weight $W^{(1)}_{ij}$ in the first layer:\n",
    "        $$ \\frac{\\partial \\hat{y}}{\\partial W^{(1)}_{ij}} = \\frac{\\partial \\hat{y}}{\\partial h^{(1)}_j} \\frac{\\partial h^{(1)}_j}{\\partial W^{(1)}_{ij}} $$\n",
    "        where $h^{(1)}_j = \\Phi(z^{(1)}_j)$ is the output of the $j$-th neuron in the first hidden layer, and $z^{(1)}_j = \\sum_l W^{(1)}_{jl} x_l + b^{(1)}_j$.\n",
    "    *   The term $\\frac{\\partial h^{(1)}_j}{\\partial W^{(1)}_{ij}}$ explicitly depends on the input $x_i$. More importantly, the upstream gradient $\\frac{\\partial \\hat{y}}{\\partial h^{(1)}_j}$ *also* depends on the path taken through the network, which is influenced by the activations of *all* neurons in subsequent layers.\n",
    "\n",
    "3.  **Input-Dependent Activations**:\n",
    "    *   For a given input `x`, each neuron in the network will have a specific activation value (before and after the ReLU).\n",
    "    *   If a ReLU neuron's input is positive, its derivative is 1, meaning it passes the gradient through. If its input is negative, its derivative is 0, effectively blocking the gradient from flowing further back through that path.\n",
    "    *   As the input `x` changes, the specific set of neurons that are \"active\" (positive pre-activation) or \"inactive\" (negative pre-activation) also changes. This alters the computational graph through which gradients flow, causing the overall gradient with respect to a parameter to vary with `x`.\n",
    "\n",
    "4.  **Local Linearization**:\n",
    "    *   The concept of \"linearized features\" in the notebook is precisely about this. While the network as a whole is non-linear, for a fixed set of parameters and a small perturbation around a specific input `x`, the network behaves approximately linearly.\n",
    "    *   The gradient $\\frac{\\partial y}{\\partial \\theta_k}(x)$ represents the \"local slope\" of the output with respect to that parameter at that specific input point `x`. Since this local slope changes depending on where you are in the input space (due to the non-linearities), the gradient itself becomes a function of `x`.\n",
    "\n",
    "In summary, the varying gradient with respect to `x` is a direct consequence of the non-linear nature of neural networks and how the chain rule propagates gradients through input-dependent activation states. This property is what allows neural networks to learn complex, non-linear relationships in data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
